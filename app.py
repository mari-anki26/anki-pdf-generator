import streamlit as st
import pandas as pd
import PyPDF2
from sudachipy import tokenizer, dictionary
from pykakasi import kakasi
from io import BytesIO

# =========================
# Configuraci√≥n de la p√°gina
# =========================
st.set_page_config(
    page_title="Generador Anki",
    page_icon="üìö",
    layout="wide"
)

# =========================
# Estilos CSS personalizados
# =========================
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        font-weight: bold;
        margin-bottom: 1rem;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# =========================
# T√≠tulo y descripci√≥n
# =========================
st.markdown('<p class="main-header">üìö Generador de Tarjetas Anki desde PDF</p>', unsafe_allow_html=True)
st.markdown("### Convierte PDFs en japon√©s en tarjetas de vocabulario para Anki")
st.markdown("---")

# Mapeo de partes de habla
POS_MAP = {
    "ÂêçË©û": "Noun",
    "ÂãïË©û": "Verb",
    "ÂΩ¢ÂÆπË©û": "Adjective",
    "ÂâØË©û": "Adverb"
}

# Inicializar tokenizador
@st.cache_resource
def init_tokenizer():
    """Inicializa el tokenizador de japon√©s"""
    try:
        tokenizer_obj = dictionary.Dictionary().create()
        return tokenizer_obj
    except Exception as e:
        st.error(f"Error al inicializar tokenizador: {e}")
        return None

@st.cache_resource
def init_kakasi():
    """Inicializa el conversor de kanji a hiragana"""
    try:
        kks = kakasi()
        return kks
    except Exception as e:
        st.error(f"Error al inicializar kakasi: {e}")
        return None

# Inicializar herramientas
with st.spinner("Cargando herramientas de procesamiento..."):
    tokenizer_obj = init_tokenizer()
    conv = init_kakasi()
    MODE = tokenizer.Tokenizer.SplitMode.C

# Funci√≥n para limpiar texto
def limpiar_texto(texto):
    """Limpia el texto eliminando espacios y saltos de l√≠nea"""
    return texto.replace("\n", "").replace(" ", "").replace("„ÄÄ", "")

# =========================
# Interfaz de usuario
# =========================

# Columnas para mejor organizaci√≥n
col1, col2 = st.columns([2, 1])

with col1:
    st.markdown("#### üìÅ Archivos requeridos")
    
    pdf_file = st.file_uploader(
        "1Ô∏è‚É£ PDF en japon√©s",
        type=["pdf"],
        help="Sube el PDF del que quieres extraer vocabulario"
    )
    
    jlpt_file = st.file_uploader(
        "2Ô∏è‚É£ CSV de niveles JLPT",
        type=["csv"],
        help="Formato: palabra,nivel (ej: È£ü„Åπ„Çã,N5)"
    )
    
    freq_file = st.file_uploader(
        "3Ô∏è‚É£ CSV de frecuencias",
        type=["csv"],
        help="Formato: palabra,frecuencia (ej: È£ü„Åπ„Çã,5000)"
    )
    
    dict_en_file = st.file_uploader(
        "4Ô∏è‚É£ CSV de diccionario JP-EN",
        type=["csv"],
        help="Formato: palabra,significado (ej: È£ü„Åπ„Çã,to eat)"
    )

with col2:
    st.markdown("#### ‚öôÔ∏è Configuraci√≥n")
    
    min_jlpt = st.selectbox(
        "Nivel JLPT m√≠nimo:",
        ["N5", "N4", "N3", "N2", "N1"],
        index=2,
        help="Solo incluir palabras hasta este nivel"
    )
    
    st.markdown("---")
    st.markdown("#### üìä Informaci√≥n")
    st.info("**Formato de los CSV:**\n\n- Primera fila: encabezados\n- Segunda columna: datos\n- Separador: coma (,)")

st.markdown("---")

# Bot√≥n de procesamiento
process = st.button("üöÄ **GENERAR TARJETAS ANKI**", type="primary", use_container_width=True)

# =========================
# Procesamiento
# =========================
if process:
    if not all([pdf_file, jlpt_file, freq_file, dict_en_file]):
        st.error("‚ö†Ô∏è Por favor, sube todos los archivos requeridos antes de procesar.")
    elif not tokenizer_obj or not conv:
        st.error("‚ùå Error: Las herramientas de procesamiento no se cargaron correctamente.")
    else:
        with st.spinner("üìñ Procesando PDF... Esto puede tomar unos minutos."):
            try:
                # Cargar CSVs
                jlpt_df = pd.read_csv(jlpt_file)
                freq_df = pd.read_csv(freq_file)
                dict_df = pd.read_csv(dict_en_file)
                
                # Crear diccionarios
                jlpt_dict = dict(zip(jlpt_df.iloc[:, 0], jlpt_df.iloc[:, 1]))
                freq_dict = dict(zip(freq_df.iloc[:, 0], freq_df.iloc[:, 1]))
                en_dict = dict(zip(dict_df.iloc[:, 0], dict_df.iloc[:, 1]))
                
                vocab = {}
                
                # Leer PDF
                pdf_file.seek(0)
                reader = PyPDF2.PdfReader(pdf_file)
                total_pages = len(reader.pages)
                
                # Barra de progreso
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for p in range(total_pages):
                    # Actualizar progreso
                    progress = (p + 1) / total_pages
                    progress_bar.progress(progress)
                    status_text.text(f"Procesando p√°gina {p + 1} de {total_pages}...")
                    
                    page = reader.pages[p]
                    raw_text = page.extract_text()
                    
                    if not raw_text:
                        continue
                    
                    clean = limpiar_texto(raw_text)
                    
                    if not clean:
                        continue
                    
                    # Tokenizar
                    tokens = tokenizer_obj.tokenize(clean, MODE)
                    
                    for t in tokens:
                        try:
                            pos_jp = t.part_of_speech()[0]
                            
                            # Filtrar part√≠culas y auxiliares
                            if pos_jp in ("Âä©Ë©û", "Âä©ÂãïË©û"):
                                continue
                            
                            base = t.dictionary_form()
                            pos = POS_MAP.get(pos_jp, "Other")
                            
                            if base not in vocab:
                                vocab[base] = {
                                    "count": 0,
                                    "pos": pos
                                }
                            vocab[base]["count"] += 1
                        except:
                            continue
                
                # Limpiar barra de progreso
                progress_bar.empty()
                status_text.empty()
                
                # Crear DataFrame
                rows = []
                jlpt_rank = {"N5": 1, "N4": 2, "N3": 3, "N2": 4, "N1": 5}
                
                for word, info in vocab.items():
                    jlpt = jlpt_dict.get(word, "")
                    
                    # Filtrar por nivel JLPT
                    if jlpt and jlpt in jlpt_rank and jlpt_rank[jlpt] > jlpt_rank[min_jlpt]:
                        continue
                    
                    # Convertir a hiragana
                    reading = conv.convert(word)
                    reading_text = "".join([item['hira'] for item in reading])
                    furigana = f"<ruby>{word}<rt>{reading_text}</rt></ruby>"
                    
                    rows.append({
                        "Front": word,
                        "Reading": reading_text,
                        "Furigana": furigana,
                        "POS": info["pos"],
                        "Meaning_EN": en_dict.get(word, ""),
                        "JLPT": jlpt,
                        "Frequency_PDF": info["count"],
                        "Frequency_Global": freq_dict.get(word, "")
                    })
                
                df = pd.DataFrame(rows)
                
                # Ordenar por JLPT y frecuencia
                df = df.sort_values(by=["JLPT", "Frequency_PDF"], ascending=[True, False])
                
                # Mostrar resultados
                st.markdown(f'<div class="success-box">‚úÖ <strong>¬°√âxito!</strong> Se generaron {len(df)} palabras √∫nicas.</div>', unsafe_allow_html=True)
                
                # Mostrar tabla
                st.markdown("### üìã Primeras 50 palabras:")
                st.dataframe(df.head(50), use_container_width=True)
                
                # Bot√≥n de descarga
                buffer = BytesIO()
                df.to_excel(buffer, index=False, engine='openpyxl')
                buffer.seek(0)
                
                st.download_button(
                    label="‚¨áÔ∏è **DESCARGAR EXCEL PARA ANKI**",
                    data=buffer,
                    file_name="anki_vocab.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    type="primary",
                    use_container_width=True
                )
                
                # Estad√≠sticas adicionales
                st.markdown("---")
                st.markdown("### üìä Estad√≠sticas")
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("Total de palabras", len(df))
                
                with col2:
                    jlpt_counts = df['JLPT'].value_counts()
                    st.metric("Niveles JLPT encontrados", len(jlpt_counts))
                
                with col3:
                    avg_freq = df['Frequency_PDF'].mean()
                    st.metric("Frecuencia promedio", f"{avg_freq:.1f}")
            
            except Exception as e:
                st.error(f"‚ùå Error durante el procesamiento: {str(e)}")
                st.exception(e)

# =========================
# Instrucciones en el sidebar
# =========================
with st.sidebar:
    st.markdown("## üìñ Gu√≠a de uso")
    st.markdown("""
    ### Pasos:
    1. Sube tu PDF en japon√©s
    2. Sube los 3 archivos CSV con datos
    3. Selecciona el nivel JLPT
    4. Haz clic en generar
    5. Descarga el Excel
    
    ### Formato de CSV:
    Los archivos CSV deben tener:
    - **Primera fila**: encabezados
    - **Primera columna**: palabra en japon√©s
    - **Segunda columna**: dato correspondiente
    
    ### Importar a Anki:
    1. Abre Anki
    2. Archivo ‚Üí Importar
    3. Selecciona el Excel descargado
    4. Configura los campos
    5. ¬°Listo! üéâ
    """)
    
    st.markdown("---")
    st.markdown("### üí° Consejos")
    st.info("Para mejores resultados, usa PDFs con texto seleccionable (no im√°genes escaneadas).")
    
    st.markdown("---")
    st.markdown("Hecho con ‚ù§Ô∏è usando Streamlit")
